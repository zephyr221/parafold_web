<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <title>
      About &middot; ParaFold
  </title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css">
  <link rel="stylesheet" href="./css/font-awesome.min.css" type="text/css">
  <link rel="stylesheet" href="./css/style.css" type="text/css">
  <link rel="shortcut icon"  type="image/x-icon" href="/assets/images/favicon.ico">
</head>

<body>
  <nav class="nav-main">
    <ul>
      <li class="logo"><a class="hvr-ripple-out" href="../index.html">P</a></li>
      <li><a class="hvr-pop button" href="../about/">About</a></li>
      <li><a href="../docs/quick-start/">Quick-Start</a></li>
      <li><a class="hvr-pop button" href="https://github.com/Zuricho/ParallelFold">GitHub</a></li>
      <li><a class="hvr-pop button" href="./">Wiki</a></li>
      <li><a class="hvr-pop button" href="../team">Our Team</a></li>
    </ul>
  </nav>

<div class="container content">
<main>
<article class="page">
  <h1 class="page-title">AlphaFold Wiki</h1>
  <p class="message">
  Hey there! This page lists a collection of useful links of AlphaFold, as well as HPC centers and GitHub issues.
</p>

<ul>
    <li><a href="#link" >Useful Links</a></li>
    <li><a href="#hpc" >HPC centers</a></li>
    <li><a href="#issue" >GitHub Issues</a></li>
    <li><a href="#video" >Video</a></li>
</ul>

          <div id="link"><h2 id="docs" class="archive__subtitle">Useful Links</h2></div>
    
          <table>
            <thead>
              <tr>
                <th>Site</th>
                <th>Introduction</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><a href="https://github.com/deepmind/alphafold/">DeepMind AlphaFold Github</a></td>
                <td>official site</td>
              </tr>
              <tr>
                <td><a href="https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb">DeepMind AlphaFold colab </a></td>
                <td>official colab</td>
              </tr>
              <tr>
                <td><a href="https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold2_advanced.ipynb">ColabFold colab</a></td>
                <td>mainteined by Sergey Ovchinnikov</td>
              </tr>
              <tr>
                <td><a href="https://www.getmoonbear.com/">MoonBear</a></td>
                <td>Use AlphaFold 2 in your browser</td>
              </tr>
              <tr>
                <td><a href="https://alphafold.ebi.ac.uk/">AlphaFold Protein Structure Database</a></td>
                <td>Developed by DeepMind and EMBL-EBI</td>
              </tr>
              <tr>
                <td><a href="https://www.af2anatomia.jp/">AlphaFold2 dismantling new book</a></td>
                <td>Created by Yoshitaka Moriwak</td>
              </tr>
            </tbody>
          </table> 

          <div id="hpc"><h2 id="docs" class="archive__subtitle">HPC centers</h2></div>
    
          <table>
            <thead>
              <tr>
                <th>HPC Centers</th>
                <th>Clusters</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><a href="https://docs.hpc.sjtu.edu.cn/app/alphafold2.html">Shanghai Jiao Tong University</a></td>
                <td>AlphaFold2 on π2.0</td>
              </tr>
              <tr>
                <td><a href="https://hpc.nih.gov/apps/alphafold2.html">NIH </a></td>
                <td>AlphaFold2 on Biowulf</td>
              </tr>
              <tr>
                <td><a href="https://wiki.unil.ch/ci/books/high-performance-computing-hpc/page/alphafold">University of Lausanne </a></td>
                <td>AlphaFold2 on DCSR</td>
              </tr>
              <tr>
                <td><a href="https://sbgrid.org//wiki/examples/alphafold2">SBGrid </a></td>
                <td>AlphaFold2 in Harvard Medical School</td>
              </tr>
              <tr>
                <td><a href="https://confluence.desy.de/display/MXW/alphafold">Deutsches Elektronen-Synchrotron DESY</a></td>
                <td>AlphaFold2 on Maxwell</td>
              </tr>
              <tr>
                <td><a href="https://helpdesk.t3.gsic.titech.ac.jp/manuals/handbook.en/freesoft/#alphafold">Tokyo Institute of Technologya </a></td>
                <td>AlphaFold2 on TSUBAME3.0</td>
              </tr>
              <tr>
                <td><a href="https://help.rc.ufl.edu/doc/AlphaFold">University of Florida </a></td>
                <td>AlphaFold2 on HiPerGator</td>
              </tr>
              <tr>
                <td><a href="https://www.rc.virginia.edu/userinfo/rivanna/software/alphafold/">University of Virginia </a></td>
                <td>AlphaFold2 on Rivanna</td>
              </tr>
              <tr>
                <td><a href="https://ccportal.ims.ac.jp/en/node/2946">RCCS Okazaki National Institute</a></td>
                <td>AlphaFold2 on cclx</td>
              </tr>
              <tr>
                <td><a href="https://wiki.metacentrum.cz/wiki/AlphaFold">Czech National Grid Infrastructure </a></td>
                <td>AlphaFold2 on MetaCentrum</td>
              </tr>
              <tr>
                <td><a href="https://www.scl.kyoto-u.ac.jp/Appli/alphafold2.html/">Kyoto University</a></td>
                <td>AlphaFold2 on SCL</td>
              </tr>
              <tr>
                <td><a href="https://biohpc.cornell.edu/lab/userguide.aspx?a=software&i=853#c">Cornell University</a></td>
                <td>AlphaFold2 on BioHPC Cloud</td>
              </tr>
              <tr>
                <td><a href="https://portal.tacc.utexas.edu/software/alphafold;jsessionid=8371C8DA73DC33BF3F787304BA6B617E">The University of Texas at Austin </a></td>
                <td>AlphaFold2 on TACC</td>
              </tr>
              <tr>
                <td><a href="https://wiki.gacrc.uga.edu/wiki/AlphaFold-Sapelo2">Georgia Advanced Computing Resource Centery</a></td>
                <td>AlphaFold2 on Sapelo2</td>
              </tr>
              <tr>
                <td><a href="https://kb.northwestern.edu/page.php?id=112835">Northwestern University</a></td>
                <td>AlphaFold2 on Quest</td>
              </tr>

            </tbody>
          </table> 
          
          <div id="issue"><h2 id="docs" class="archive__subtitle">GitHub Issues</h2></div>

          <p><a href="https://github.com/deepmind/alphafold/issues/5">Issue 5: Database disk type</a></p>
          <div class="message">
            (<a href="https://github.com/Augustin-Zidek">Augustin-Zidek</a>) The genetic search tools are very IO intensive, hence having an SSD helps.<br><br>
            For more details see e.g. the HH Suite wiki that discusses HHBlits performance: <a href="https://github.com/soedinglab/hh-suite/wiki#running-hhblits-efficiently-on-a-computer-cluster">https://github.com/soedinglab/hh-suite/wiki#running-hhblits-efficiently-on-a-computer-cluster</a>
          </div>

          <p><a href="https://github.com/deepmind/alphafold/issues/6">Issue 6: How long does it take on T1050 (779 residues)</a></p>
          <div class="message">
            (<a href="https://github.com/Augustin-Zidek">Augustin-Zidek</a>) This is hard to answer without more context, especially without knowing the speed of your CPU and your hard drive (whether it is an SSD or an HDD).<br><br>
            But in general, you can expect the time to grow with the length of the protein and the MSA search taking up to a few hours with a slow disk / CPU.<br><br>
            For the actual folding (i.e. running the AlphaFold model), the disk speed doesn't matter anymore, what matters is whether you are using a GPU and its performance.
          </div>

          <p><a href="https://github.com/deepmind/alphafold/issues/9">Issue 9: AlphaFold's speed</a></p>
          <div class="message">
            (<a href="https://github.com/tfgg">tfgg</a>) If you read the Nature paper, you'll see that AlphaFold 2 is more accurate, and the GPU times are in fact very fast: 0.6 minutes at 256 residues, 1.1 minutes at 384 residues, and 2.1 hours at 2,500 residues. These appear to be comparable to or faster than RoseTTAFold.
          </div>
          
          <p><a href="https://github.com/deepmind/alphafold/issues/12">Issue 12: GPU required?</a></p>
          <div class="message">
            (<a href="https://github.com/tfgg">tfgg</a>) You can run without a GPU (with the --use_gpu=False flag) but it'll be much slower.<br><br>

            (<a href="https://github.com/tfgg">tfgg</a>) You can run outside Docker without GPU by setting CUDA_VISIBLE_DEVICES to be empty (0 will correspond to the first GPU). If your machine doesn't have a GPU, this won't be necessary and it will run on CPU.

            (<a href="https://github.com/huhlim">huhlim</a>) For the timing using CPU, it took more than predictions using GPUs. For a ~140 residue protein, it took ~25 minutes x 5 models for model buildings + ~25 minutes for the input feature generation. I have two Intel Xeon Silver 4214 @ 2.2GHz (24 threads for each) on a node. I have no idea how many threads were used for the inference.
          </div>

          <p><a href="https://github.com/deepmind/alphafold/issues/20">Issue 20: jaxlib version</a></p>
          <div class="message">
            (<a href="https://github.com/tfgg">tfgg</a>) We require version of 0.1.69 jaxlib to be able to use CUDA unified memory for running long sequences. If you don't need this you can probably run with 0.1.68, but that might be related to the illegal address error that you see. 
          </div>


          <p><a href="https://github.com/deepmind/alphafold/issues/53">Issue 53: GDT and lDDT Scores</a></p>
          <div class="message">
            (<a href="https://github.com/abridgland">abridgland</a>) There are a number of external tools available for computing these metrics. For GDT consider using LGA <a href="http://proteinmodel.org/AS2TS/LGA/lga.html">(http://proteinmodel.org/AS2TS/LGA/lga.html)</a> and for lDDT consider the SWISS-MODEL server <a href="https://swissmodel.expasy.org/lddt">(https://swissmodel.expasy.org/lddt)</a>. These tools are reference implementations for their respective metrics. Please note that the lDDT scores will be computed on all atoms, not just C-alpha atoms. When we use the latter in our paper it is referred to as lDDT-Ca. 
          </div>

          <p><a href="https://github.com/deepmind/alphafold/issues/61">Issue 61: predicted TM-score (pTM)</a></p>
          <div class="message">
            (<a href="https://github.com/abridgland">abridgland</a>) The *_ptm models were fine-tuned from the non-pTM models (see section 1.9.7 in the supplementary information of our paper). This is why the outputs from these models do not match exactly.<br><br>

            We recommend running the non-pTM models for structure prediction because these were used in CASP14 and have been the most thoroughly validated. We think that the pTM models are very slightly worse than the regular models (around 0.5 GDT on CASP14). You can also run one of the pTM models separately in order to get predicted aligned error. This is the protocol we use in our Colab notebook (we choose model_2_ptm for predicted aligned errors).
          </div>


          <p><a href="https://github.com/deepmind/alphafold/issues/30">Issue 30&66: Distribution over multiple GPUs</a></p>
          <div class="message">
            (<a href="https://github.com/abridgland">abridgland</a>) This code is not designed to make use of more than one GPU.<br><br>

            (<a href="https://github.com/tfgg">tfgg</a>) We don't parallelize the model itself in JAX over multiple GPUs, but we do enable unified memory which should (CUDA might be smarter) at least be able to use the host RAM as well. 
          </div>

          <p><a href="https://github.com/deepmind/alphafold/issues/74">Issue 74: Speed up prediction</a></p>
          <div class="message">
            (<a href="https://github.com/abridgland">abridgland</a>) See <a href=">https://github.com/deepmind/alphafold#inferencing-many-proteins">https://github.com/deepmind/alphafold#inferencing-many-proteins</a> for advice on how to avoid re-compilations when inferencing many proteins if that is relevant to your use case.<br><br>

            In general having more GPU memory won’t help speed up model inference unless you’re finding the GPU memory is full (i.e. you’re overflowing to host RAM). However you may be able to improve speed a bit by using a larger subbatch size: alphafold/alphafold/model/config.py Line 325<br><br>

            <code class="language-plaintext highlighter-rouge">&lt;'subbatch_size': 4,&gt;</code> <br><br>
            which trades memory for speed. Picking the right value will depend on your inputs and hardware. <br><br>
            
            Finally, depending on how much you care about accuracy, you could simply run just 1 or 2 models rather than all 5. I hope that helps!
          </div>

          <p><a href="https://github.com/deepmind/alphafold/issues/30">Issue 30&66: Distribution over multiple GPUs</a></p>
          <div class="message">
            (<a href="https://github.com/abridgland">abridgland</a>) This code is not designed to make use of more than one GPU.<br><br>

            (<a href="https://github.com/tfgg">tfgg</a>) We don't parallelize the model itself in JAX over multiple GPUs, but we do enable unified memory which should (CUDA might be smarter) at least be able to use the host RAM as well. 
          </div>

          <p><a href="https://github.com/deepmind/alphafold/issues/30">Issue 30&66: Distribution over multiple GPUs</a></p>
          <div class="message">
            (<a href="https://github.com/abridgland">abridgland</a>) This code is not designed to make use of more than one GPU.<br><br>

            (<a href="https://github.com/tfgg">tfgg</a>) We don't parallelize the model itself in JAX over multiple GPUs, but we do enable unified memory which should (CUDA might be smarter) at least be able to use the host RAM as well. 
          </div>













          <code class="language-plaintext highlighter-rouge">&lt;code&gt;</code> 


















          
          
          <div id="video"><h2 id="docs" class="archive__subtitle">Video</h2></div>
          <p>钟博子韬 2021-07-22 </p>
          <p><a href="https://ins.sjtu.edu.cn/seminars/1959">《Alphafold2: 如何应用AI预测蛋白质三维结构》</a></p>






</article>

</main>

<footer class="footer">
  <small>
      <span class="copyright"><i class="fa fa-copyright"></i>Copyright 2021 SJTU Network & Information Center All rights reserved. （沪交ICP备20210257）</span>
  </small>
  <div class="ftr-links">
    <a href="https://github.com/Zuricho/ParallelFold"><i class="fa fa-github-alt"></i></a>
  </div>
</footer>



</body>
</html>
